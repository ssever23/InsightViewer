{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Vector Store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# additional imports\n",
    "import os\n",
    "import logging\n",
    "from uuid import uuid4 # for unique ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vector Store Chroma**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load and split documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 100, add_start_index = False) # splits the text into chunks\n",
    "\n",
    "# Load and split documents\n",
    "def load_pdfs(pdf_path):\n",
    "    chunks = []\n",
    "    if not os.path.exists(pdf_path):\n",
    "        logging.error(f\"Filepath {pdf_path} does not exist\")\n",
    "        return chunks\n",
    "    \n",
    "    for file in os.listdir(pdf_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(pdf_path, file)\n",
    "            try:\n",
    "                loader = PyPDFLoader(filepath)\n",
    "                docs = loader.load()\n",
    "                split_texts = text_splitter.split_documents(docs)\n",
    "                chunks.extend(split_texts)\n",
    "                logging.info(f\"Processed {filepath} successfully with {len(split_texts)} chunks\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to process {filepath}: {e}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks\n",
    "pdf_path = \"/home/ssever/rag-llm-demo/data/files\"\n",
    "chunks = load_pdfs(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L12-v2\")\n",
    "\n",
    "# Load chunks into the vector store\n",
    "destination = \"/home/ssever/rag-llm-demo/data/vector_store/Chroma\"\n",
    "db = Chroma.from_documents(chunks, embedding_function, persist_directory=destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Test query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Taking social and environmental responsibility for all we do is an integral part of how we perceive ourselves as a company.\"\n",
    "docs = db.similarity_search_with_score(query, k=5)\n",
    "\n",
    "found_chunks = []\n",
    "\n",
    "for doc, score in docs:\n",
    "    found_chunks.append(doc.page_content)\n",
    "    \n",
    "found_chunks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
