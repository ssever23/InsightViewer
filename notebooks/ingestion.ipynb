{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Embeddings / Vector store (Chroma via LangChain) --------\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# -------- Parsers --------\n",
    "import pymupdf4llm  # PDF -> Markdown\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "import pdfplumber  # table extraction\n",
    "from pypdf import PdfReader  # quick full-text as-needed\n",
    "from docx import Document as DocxDocument\n",
    "from dateutil import parser as dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427757ea",
   "metadata": {},
   "source": [
    "### **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f469c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATA_DIR = \"/home/ssever/InsightViewer/data/reports\"\n",
    "STORAGE_DIR = \"/home/ssever/InsightViewer/storage\"\n",
    "#STORAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHROMA_DIR = STORAGE_DIR + \"/chroma\"\n",
    "COLLECTION_NAME = \"filings\"\n",
    "\n",
    "DUCKDB_PATH = STORAGE_DIR + \"/metrics.duckdb\"\n",
    "#PLOTS_DIR = STORAGE_DIR + \"/plots\"\n",
    "#PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L12-v2 \"\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9bbbf",
   "metadata": {},
   "source": [
    "### **Metadata extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f517c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_FORM = re.compile(r\"\\bForm\\s+(10[-\\s]?K|10[-\\s]?Q|8[-\\s]?K)\\b\", re.I)\n",
    "RE_AR   = re.compile(r\"\\b(Annual\\s+Report)\\b\", re.I)\n",
    "RE_FY_ENDED  = re.compile(r\"\\bfiscal\\s+year\\s+ended\\s+([A-Za-z0-9, ]+)\\b\", re.I)\n",
    "RE_Q_ENDED   = re.compile(r\"\\bquarterly\\s+period\\s+ended\\s+([A-Za-z0-9, ]+)\\b\", re.I)\n",
    "RE_FY_CODE   = re.compile(r\"\\bFY(?:20)?(\\d{2})\\b\", re.I)\n",
    "RE_Q_CODE    = re.compile(r\"\\bQ([1-4])\\b\", re.I)\n",
    "RE_COMPANY   = re.compile(r\"\\b([A-Z][A-Za-z&.,()\\- ]{2,}(?:Corporation|Company|Inc\\.|Incorporated|PLC))\\b\")\n",
    "\n",
    "TICKER_PREFIX_RE = re.compile(r\"^([A-Za-z]{1,6})[ _-]\", re.I)\n",
    "FILENAME_YEAR_RE = re.compile(r\"(20\\d{2}|FY(?:20)?\\d{2})\", re.I)\n",
    "FILING_NAME_RE  = re.compile(r\"(10[-_]?K|10[-_]?Q|8-K|AR|Annual[_-]?Report|PressRelease|Slides|Transcript|Outlook)\", re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625c50f",
   "metadata": {},
   "source": [
    "### **Table extraction + tidy facts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_METRICS = {\n",
    "    \"revenue\": [\"revenue\", \"net sales\", \"total revenue\", \"sales\"],\n",
    "    \"net_income\": [\n",
    "        \"net income\", \"net income attributable to\", \"profit for the year\",\n",
    "        \"net earnings\", \"consolidated net income\", \"net profit\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def scan_units_scale(text: str) -> Tuple[Optional[str], Optional[int]]:\n",
    "    t = text.lower()\n",
    "    units = \"USD\" if (\"$\" in t or \"usd\" in t or \"dollars\" in t) else None\n",
    "    scale = None\n",
    "    if \"in millions\" in t:\n",
    "        scale = 1_000_000\n",
    "    elif \"in thousands\" in t or \"in 000s\" in t:\n",
    "        scale = 1_000\n",
    "    return units, scale\n",
    "\n",
    "\n",
    "def parse_number(cell: Any) -> Optional[float]:\n",
    "    s = str(cell).strip()\n",
    "    if s in (\"\", \"-\", \"—\", \"–\"):\n",
    "        return None\n",
    "    s = s.replace(\",\", \"\")\n",
    "    negative = s.startswith(\"(\") and s.endswith(\")\")\n",
    "    if negative:\n",
    "        s = s[1:-1]\n",
    "    s = re.sub(r\"[\\$\\€\\£]|[^\\d\\.\\-]\", \"\", s)\n",
    "    if not s or s in (\".\", \"-\"):\n",
    "        return None\n",
    "    try:\n",
    "        val = float(s)\n",
    "        return -val if negative else val\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def best_metric_match(label: str, threshold: int = 80) -> Optional[str]:\n",
    "    from rapidfuzz import fuzz, process\n",
    "    label_norm = re.sub(r\"\\s+\", \" \", label.lower()).strip()\n",
    "    candidates: List[Tuple[str, int]] = []\n",
    "    for norm, synonyms in TARGET_METRICS.items():\n",
    "        best = process.extractOne(label_norm, synonyms, scorer=fuzz.token_sort_ratio)\n",
    "        if best and best[1] >= threshold:\n",
    "            candidates.append((norm, best[1]))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[0][0]\n",
    "\n",
    "\n",
    "def extract_pdf_tables_with_provenance(path: Path) -> List[Dict[str, Any]]:\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with pdfplumber.open(str(path)) as pdf:\n",
    "        for p_idx, page in enumerate(pdf.pages):\n",
    "            try:\n",
    "                tables = page.extract_tables() or []\n",
    "            except Exception:\n",
    "                tables = []\n",
    "            for t_idx, table in enumerate(tables):\n",
    "                if not table or len(table) < 2:\n",
    "                    continue\n",
    "                df = pd.DataFrame(table)\n",
    "                header = df.iloc[0].astype(str).tolist()\n",
    "                header_has_text = sum(bool(re.search(r\"[A-Za-z]\", c or \"\")) for c in header) >= 2\n",
    "                if header_has_text:\n",
    "                    df.columns = header\n",
    "                    df = df.iloc[1:].reset_index(drop=True)\n",
    "                out.append({\n",
    "                    \"page\": p_idx + 1,\n",
    "                    \"table_id\": t_idx + 1,\n",
    "                    \"df\": df,\n",
    "                })\n",
    "    return out\n",
    "\n",
    "\n",
    "def melt_wide_years(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"label\", \"year\", \"value\"])\n",
    "\n",
    "    # Always work by column index to avoid duplicate-name issues\n",
    "    ncols = df.shape[1]\n",
    "    # Helper: get a Series for column i even if names are duplicated\n",
    "    def col_series(i: int) -> pd.Series:\n",
    "        ser = df.iloc[:, i]\n",
    "        # If iloc gives a DataFrame (shouldn't), take first col\n",
    "        if isinstance(ser, pd.DataFrame):\n",
    "            ser = ser.iloc[:, 0]\n",
    "        return ser\n",
    "\n",
    "    # 1) pick a likely label column\n",
    "    label_idx = 0\n",
    "    for i in range(ncols):\n",
    "        ser = col_series(i).astype(str).fillna(\"\")\n",
    "        # count entries that look textual\n",
    "        text_count = ser.str.contains(r\"[A-Za-z]\", regex=True, na=False).sum()\n",
    "        if text_count >= max(2, int(0.2 * len(ser))):\n",
    "            label_idx = i\n",
    "            break\n",
    "\n",
    "    # 2) detect year columns by header text\n",
    "    year_idxs = []\n",
    "    for i in range(ncols):\n",
    "        if i == label_idx:\n",
    "            continue\n",
    "        header = str(df.columns[i])\n",
    "        if re.search(r\"(19|20)\\d{2}\", header):\n",
    "            year_idxs.append(i)\n",
    "\n",
    "    if not year_idxs:\n",
    "        return pd.DataFrame(columns=[\"label\", \"year\", \"value\"])\n",
    "\n",
    "    # 3) build a compact DataFrame with one label + many year columns\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp[\"label\"] = col_series(label_idx).astype(str)\n",
    "\n",
    "    # Use distinct temporary column names to avoid duplicate header collisions\n",
    "    year_col_map = {}\n",
    "    for i in year_idxs:\n",
    "        header = str(df.columns[i])\n",
    "        # extract the 4-digit year token\n",
    "        m = re.search(r\"(19|20)\\d{2}\", header)\n",
    "        year = m.group(0) if m else header\n",
    "        colname = f\"y_{year}\"\n",
    "        # de-duplicate if same year appears multiple times\n",
    "        k = 2\n",
    "        base = colname\n",
    "        while colname in tmp.columns:\n",
    "            colname = f\"{base}_{k}\"\n",
    "            k += 1\n",
    "        tmp[colname] = col_series(i)\n",
    "\n",
    "        # remember which colname maps to which year\n",
    "        year_col_map[colname] = year\n",
    "\n",
    "    # 4) melt to tidy\n",
    "    tidy = tmp.melt(id_vars=[\"label\"], var_name=\"year_col\", value_name=\"raw_value\")\n",
    "\n",
    "    # 5) map back to the actual year and parse numbers\n",
    "    tidy[\"year\"] = tidy[\"year_col\"].map(year_col_map).fillna(\"\")\n",
    "    tidy.dropna(subset=[\"year\"], inplace=True)\n",
    "    tidy[\"value\"] = tidy[\"raw_value\"].apply(parse_number)\n",
    "\n",
    "    # 6) final clean-up\n",
    "    tidy = tidy[[\"label\", \"year\", \"value\"]].reset_index(drop=True)\n",
    "    return tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma(collection_name: str = COLLECTION_NAME, persist_dir: str = CHROMA_DIR):\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "    vs = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    return vs\n",
    "\n",
    "\n",
    "def upsert_chunks_to_chroma(\n",
    "    chunks,\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    persist_dir: str = CHROMA_DIR,\n",
    "):\n",
    "    if not chunks:\n",
    "        return 0\n",
    "\n",
    "    vs = get_chroma(collection_name, persist_dir)\n",
    "\n",
    "    # Build Documents then filter complex metadata\n",
    "    docs = [Document(page_content=c[\"text\"], metadata=c[\"metadata\"]) for c in chunks]\n",
    "    docs = filter_complex_metadata(docs)\n",
    "\n",
    "    vs.add_documents(docs)\n",
    "\n",
    "    # For older langchain versions that still have .persist(), this is harmless.\n",
    "    try:\n",
    "        vs.persist()  # no-op on modern versions / AttributeError otherwise\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return len(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vizgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
