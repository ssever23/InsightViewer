{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Embeddings / Vector store (Chroma via LangChain) --------\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# -------- Parsers --------\n",
    "import pymupdf4llm  # PDF -> Markdown\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "import pdfplumber  # table extraction\n",
    "from pypdf import PdfReader  # quick full-text as-needed\n",
    "from docx import Document as DocxDocument\n",
    "from dateutil import parser as dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427757ea",
   "metadata": {},
   "source": [
    "### **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATA_DIR = \"/home/ssever/InsightViewer/data/test\"\n",
    "STORAGE_DIR = \"/home/ssever/InsightViewer/test_storage\"\n",
    "#STORAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHROMA_DIR = STORAGE_DIR + \"/chroma\"\n",
    "COLLECTION_NAME = \"filings\"\n",
    "\n",
    "DUCKDB_PATH = STORAGE_DIR + \"/metrics.duckdb\"\n",
    "#PLOTS_DIR = STORAGE_DIR + \"/plots\"\n",
    "#PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L12-v2\"\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e9bbbf",
   "metadata": {},
   "source": [
    "### **Metadata extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f517c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_TO_TICKER = {\n",
    "    \"Microsoft Corporation\": \"MSFT\",\n",
    "}\n",
    "\n",
    "RE_FORM = re.compile(r\"\\bForm\\s+(10[-\\s]?K|10[-\\s]?Q|8[-\\s]?K)\\b\", re.I)\n",
    "RE_AR   = re.compile(r\"\\b(Annual\\s+Report)\\b\", re.I)\n",
    "RE_FY_ENDED  = re.compile(r\"\\bfiscal\\s+year\\s+ended\\s+([A-Za-z0-9, ]+)\\b\", re.I)\n",
    "RE_Q_ENDED   = re.compile(r\"\\bquarterly\\s+period\\s+ended\\s+([A-Za-z0-9, ]+)\\b\", re.I)\n",
    "RE_FY_CODE   = re.compile(r\"\\bFY(?:20)?(\\d{2})\\b\", re.I)\n",
    "RE_Q_CODE    = re.compile(r\"\\bQ([1-4])\\b\", re.I)\n",
    "RE_COMPANY   = re.compile(r\"\\b([A-Z][A-Za-z&.,()\\- ]{2,}(?:Corporation|Company|Inc\\.|Incorporated|PLC))\\b\")\n",
    "\n",
    "TICKER_PREFIX_RE = re.compile(r\"^([A-Za-z]{1,6})[ _-]\", re.I)\n",
    "FILENAME_YEAR_RE = re.compile(r\"(20\\d{2}|FY(?:20)?\\d{2})\", re.I)\n",
    "FILING_NAME_RE  = re.compile(r\"(10[-_]?K|10[-_]?Q|8-K|AR|Annual[_-]?Report|PressRelease|Slides|Transcript|Outlook)\", re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936eba3",
   "metadata": {},
   "source": [
    "### **Content-first metadata extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_form(form_str: str) -> str:\n",
    "    s = form_str.upper().replace(\" \", \"\").replace(\"_\", \"\").replace(\"-\", \"\")\n",
    "    if s == \"10K\": return \"10-K\"\n",
    "    if s == \"10Q\": return \"10-Q\"\n",
    "    if s == \"8K\":  return \"8-K\"\n",
    "    return form_str.upper()\n",
    "\n",
    "\n",
    "def _year_from_datephrase(phrase: str) -> Optional[str]:\n",
    "    try:\n",
    "        dt = dateparser.parse(phrase, fuzzy=True)\n",
    "        return str(dt.year)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _read_pdf_pages_quick(path: Path, max_pages: int = 6) -> List[str]:\n",
    "    txts: List[str] = []\n",
    "    try:\n",
    "        r = PdfReader(str(path))\n",
    "        for p in r.pages[:max_pages]:\n",
    "            try:\n",
    "                txts.append(p.extract_text() or \"\")\n",
    "            except Exception:\n",
    "                txts.append(\"\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return txts\n",
    "\n",
    "\n",
    "def _read_docx_quick(path: Path, max_chars: int = 12000) -> List[str]:\n",
    "    try:\n",
    "        doc = DocxDocument(str(path))\n",
    "        text = \"\\n\".join(p.text for p in doc.paragraphs if p.text)[:max_chars]\n",
    "        chunk = 3000\n",
    "        return [text[i:i+chunk] for i in range(0, len(text), chunk)][:4]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_meta_from_content(path: Path) -> Dict[str, Any]:\n",
    "    pages = _read_pdf_pages_quick(path) if path.suffix.lower() == \".pdf\" else (\n",
    "        _read_docx_quick(path) if path.suffix.lower() in {\".docx\", \".doc\"} else []\n",
    "    )\n",
    "    filing_type = None\n",
    "    fiscal_year = None\n",
    "    fiscal_period = None\n",
    "    company_name = None\n",
    "    prov: Dict[str, Any] = {}\n",
    "    conf = 0.0\n",
    "\n",
    "    for idx, t in enumerate(pages, start=1):\n",
    "        if not t.strip():\n",
    "            continue\n",
    "        m_form = RE_FORM.search(t)\n",
    "        if m_form and not filing_type:\n",
    "            filing_type = _norm_form(m_form.group(1))\n",
    "            prov.setdefault(\"filing_type\", {\"page\": idx, \"evidence\": m_form.group(0)})\n",
    "            conf = max(conf, 0.8)\n",
    "        if not filing_type and RE_AR.search(t):\n",
    "            filing_type = \"AR\"\n",
    "            prov.setdefault(\"filing_type\", {\"page\": idx, \"evidence\": \"Annual Report\"})\n",
    "            conf = max(conf, 0.6)\n",
    "\n",
    "        if not fiscal_year:\n",
    "            fy_phrase = RE_FY_ENDED.search(t)\n",
    "            if fy_phrase:\n",
    "                y = _year_from_datephrase(fy_phrase.group(1))\n",
    "                if y:\n",
    "                    fiscal_year = y\n",
    "                    prov.setdefault(\"fiscal_year\", {\"page\": idx, \"evidence\": fy_phrase.group(0)})\n",
    "                    conf = max(conf, 0.7)\n",
    "        if not fiscal_period:\n",
    "            q_phrase = RE_Q_ENDED.search(t)\n",
    "            if q_phrase:\n",
    "                q = RE_Q_CODE.search(t)\n",
    "                if q:\n",
    "                    fiscal_period = f\"Q{q.group(1)}\"\n",
    "                    prov.setdefault(\"fiscal_period\", {\"page\": idx, \"evidence\": q.group(0)})\n",
    "                    conf = max(conf, 0.6)\n",
    "                if not fiscal_year:\n",
    "                    y = _year_from_datephrase(q_phrase.group(1))\n",
    "                    if y:\n",
    "                        fiscal_year = y\n",
    "                        prov.setdefault(\"fiscal_year\", {\"page\": idx, \"evidence\": q_phrase.group(0)})\n",
    "                        conf = max(conf, 0.6)\n",
    "\n",
    "        if not fiscal_year:\n",
    "            m_fy = RE_FY_CODE.search(t)\n",
    "            if m_fy:\n",
    "                fiscal_year = \"20\" + m_fy.group(1)\n",
    "                prov.setdefault(\"fiscal_year\", {\"page\": idx, \"evidence\": m_fy.group(0)})\n",
    "                conf = max(conf, 0.55)\n",
    "        if not fiscal_period:\n",
    "            m_q = RE_Q_CODE.search(t)\n",
    "            if m_q:\n",
    "                fiscal_period = f\"Q{m_q.group(1)}\"\n",
    "                prov.setdefault(\"fiscal_period\", {\"page\": idx, \"evidence\": m_q.group(0)})\n",
    "                conf = max(conf, 0.5)\n",
    "\n",
    "        if not company_name:\n",
    "            m_co = RE_COMPANY.search(t)\n",
    "            if m_co:\n",
    "                company_name = m_co.group(0).replace(\"  \", \" \").strip().rstrip(\",\")\n",
    "                prov.setdefault(\"company_name\", {\"page\": idx, \"evidence\": company_name})\n",
    "                conf = max(conf, 0.5)\n",
    "\n",
    "        if filing_type and fiscal_year and company_name and conf >= 0.8:\n",
    "            break\n",
    "\n",
    "    ticker = COMPANY_TO_TICKER.get(company_name)\n",
    "    if ticker:\n",
    "        prov.setdefault(\"ticker\", {\"evidence\": f\"map:{company_name}->{ticker}\"})\n",
    "        conf = max(conf, 0.85)\n",
    "\n",
    "    return {\n",
    "        \"ticker\": ticker,\n",
    "        \"filing_type\": filing_type,\n",
    "        \"fiscal_year\": fiscal_year,\n",
    "        \"fiscal_period\": fiscal_period,\n",
    "        \"company_name\": company_name,\n",
    "        \"meta_confidence\": conf,\n",
    "        \"meta_provenance\": prov,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_document_meta(path: Path, ticker_default: Optional[str] = None) -> Dict[str, Any]:\n",
    "    stem = path.stem\n",
    "    m_tick = TICKER_PREFIX_RE.match(stem)\n",
    "    filename_ticker = m_tick.group(1).upper() if m_tick else (ticker_default.upper() if ticker_default else None)\n",
    "    m_file = FILING_NAME_RE.search(stem)\n",
    "    filename_filing = _norm_form(m_file.group(1)) if m_file else None\n",
    "    m_year = FILENAME_YEAR_RE.search(stem)\n",
    "    filename_year = None\n",
    "    if m_year:\n",
    "        y = m_year.group(1).upper()\n",
    "        if y.startswith(\"FY\"):\n",
    "            digits = re.sub(r\"[^0-9]\", \"\", y[2:])\n",
    "            filename_year = (\"20\" + digits) if len(digits) == 2 else (digits if len(digits) == 4 else None)\n",
    "        else:\n",
    "            filename_year = y\n",
    "\n",
    "    content_meta = extract_meta_from_content(path)\n",
    "\n",
    "    return {\n",
    "        \"ticker\": content_meta.get(\"ticker\") or filename_ticker,\n",
    "        \"filing_type\": content_meta.get(\"filing_type\") or filename_filing,\n",
    "        \"fiscal_year\": content_meta.get(\"fiscal_year\") or filename_year,\n",
    "        \"fiscal_period\": content_meta.get(\"fiscal_period\"),\n",
    "        \"company_name\": content_meta.get(\"company_name\"),\n",
    "        \"meta_confidence\": content_meta.get(\"meta_confidence\"),\n",
    "        \"meta_provenance\": content_meta.get(\"meta_provenance\"),\n",
    "        \"source_filename\": path.name,\n",
    "        \"source_path\": str(path),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8874b0",
   "metadata": {},
   "source": [
    "### **Markdown parsing and chunking (text → Chroma)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_markdown(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        return pymupdf4llm.to_markdown(str(pdf_path))\n",
    "    except Exception:\n",
    "        try:\n",
    "            docs = UnstructuredPDFLoader(str(pdf_path), strategy=\"hi_res\").load()\n",
    "            return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "        except Exception:\n",
    "            raw = []\n",
    "            try:\n",
    "                reader = PdfReader(str(pdf_path))\n",
    "                for p in reader.pages:\n",
    "                    raw.append(p.extract_text() or \"\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            return \"\\n\\n\".join(raw)\n",
    "\n",
    "\n",
    "def docx_to_markdown(docx_path: Path) -> str:\n",
    "    docs = UnstructuredWordDocumentLoader(str(docx_path)).load()\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "def md_to_chunks(markdown_text: str, base_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    headers_to_split_on = [(\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\")]\n",
    "    header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_docs = header_splitter.split_text(markdown_text)\n",
    "\n",
    "    chunks: List[Dict[str, Any]] = []\n",
    "    for d in md_docs:\n",
    "        content = d.page_content\n",
    "        meta = {**base_metadata, **d.metadata}\n",
    "        has_table = bool(re.search(r\"(^|\\n)\\s*\\|.+\\|\\s*(\\n|$)\", content))\n",
    "        meta[\"has_table\"] = has_table\n",
    "\n",
    "        if has_table or len(content) <= CHUNK_SIZE:\n",
    "            chunks.append({\"text\": content, \"metadata\": meta})\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "            )\n",
    "            for t in splitter.split_text(content):\n",
    "                chunks.append({\"text\": t, \"metadata\": meta})\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625c50f",
   "metadata": {},
   "source": [
    "### **Table extraction + tidy facts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_METRICS = {\n",
    "    \"revenue\": [\"revenue\", \"net sales\", \"total revenue\", \"sales\"],\n",
    "    \"net_income\": [\n",
    "        \"net income\", \"net income attributable to\", \"profit for the year\",\n",
    "        \"net earnings\", \"consolidated net income\", \"net profit\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def scan_units_scale(text: str) -> Tuple[Optional[str], Optional[int]]:\n",
    "    t = text.lower()\n",
    "    units = \"USD\" if (\"$\" in t or \"usd\" in t or \"dollars\" in t) else None\n",
    "    scale = None\n",
    "    if \"in millions\" in t:\n",
    "        scale = 1_000_000\n",
    "    elif \"in thousands\" in t or \"in 000s\" in t:\n",
    "        scale = 1_000\n",
    "    return units, scale\n",
    "\n",
    "\n",
    "def parse_number(cell: Any) -> Optional[float]:\n",
    "    s = str(cell).strip()\n",
    "    if s in (\"\", \"-\", \"—\", \"–\"):\n",
    "        return None\n",
    "    s = s.replace(\",\", \"\")\n",
    "    negative = s.startswith(\"(\") and s.endswith(\")\")\n",
    "    if negative:\n",
    "        s = s[1:-1]\n",
    "    s = re.sub(r\"[\\$\\€\\£]|[^\\d\\.\\-]\", \"\", s)\n",
    "    if not s or s in (\".\", \"-\"):\n",
    "        return None\n",
    "    try:\n",
    "        val = float(s)\n",
    "        return -val if negative else val\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def best_metric_match(label: str, threshold: int = 80) -> Optional[str]:\n",
    "    from rapidfuzz import fuzz, process\n",
    "    label_norm = re.sub(r\"\\s+\", \" \", label.lower()).strip()\n",
    "    candidates: List[Tuple[str, int]] = []\n",
    "    for norm, synonyms in TARGET_METRICS.items():\n",
    "        best = process.extractOne(label_norm, synonyms, scorer=fuzz.token_sort_ratio)\n",
    "        if best and best[1] >= threshold:\n",
    "            candidates.append((norm, best[1]))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[0][0]\n",
    "\n",
    "\n",
    "def extract_pdf_tables_with_provenance(path: Path) -> List[Dict[str, Any]]:\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    with pdfplumber.open(str(path)) as pdf:\n",
    "        for p_idx, page in enumerate(pdf.pages):\n",
    "            try:\n",
    "                tables = page.extract_tables() or []\n",
    "            except Exception:\n",
    "                tables = []\n",
    "            for t_idx, table in enumerate(tables):\n",
    "                if not table or len(table) < 2:\n",
    "                    continue\n",
    "                df = pd.DataFrame(table)\n",
    "                header = df.iloc[0].astype(str).tolist()\n",
    "                header_has_text = sum(bool(re.search(r\"[A-Za-z]\", c or \"\")) for c in header) >= 2\n",
    "                if header_has_text:\n",
    "                    df.columns = header\n",
    "                    df = df.iloc[1:].reset_index(drop=True)\n",
    "                out.append({\n",
    "                    \"page\": p_idx + 1,\n",
    "                    \"table_id\": t_idx + 1,\n",
    "                    \"df\": df,\n",
    "                })\n",
    "    return out\n",
    "\n",
    "\n",
    "def melt_wide_years(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"label\", \"year\", \"value\"])\n",
    "\n",
    "    # Always work by column index to avoid duplicate-name issues\n",
    "    ncols = df.shape[1]\n",
    "    # Helper: get a Series for column i even if names are duplicated\n",
    "    def col_series(i: int) -> pd.Series:\n",
    "        ser = df.iloc[:, i]\n",
    "        # If iloc gives a DataFrame (shouldn't), take first col\n",
    "        if isinstance(ser, pd.DataFrame):\n",
    "            ser = ser.iloc[:, 0]\n",
    "        return ser\n",
    "\n",
    "    # 1) pick a likely label column\n",
    "    label_idx = 0\n",
    "    for i in range(ncols):\n",
    "        ser = col_series(i).astype(str).fillna(\"\")\n",
    "        # count entries that look textual\n",
    "        text_count = ser.str.contains(r\"[A-Za-z]\", regex=True, na=False).sum()\n",
    "        if text_count >= max(2, int(0.2 * len(ser))):\n",
    "            label_idx = i\n",
    "            break\n",
    "\n",
    "    # 2) detect year columns by header text\n",
    "    year_idxs = []\n",
    "    for i in range(ncols):\n",
    "        if i == label_idx:\n",
    "            continue\n",
    "        header = str(df.columns[i])\n",
    "        if re.search(r\"(19|20)\\d{2}\", header):\n",
    "            year_idxs.append(i)\n",
    "\n",
    "    if not year_idxs:\n",
    "        return pd.DataFrame(columns=[\"label\", \"year\", \"value\"])\n",
    "\n",
    "    # 3) build a compact DataFrame with one label + many year columns\n",
    "    tmp = pd.DataFrame()\n",
    "    tmp[\"label\"] = col_series(label_idx).astype(str)\n",
    "\n",
    "    # Use distinct temporary column names to avoid duplicate header collisions\n",
    "    year_col_map = {}\n",
    "    for i in year_idxs:\n",
    "        header = str(df.columns[i])\n",
    "        # extract the 4-digit year token\n",
    "        m = re.search(r\"(19|20)\\d{2}\", header)\n",
    "        year = m.group(0) if m else header\n",
    "        colname = f\"y_{year}\"\n",
    "        # de-duplicate if same year appears multiple times\n",
    "        k = 2\n",
    "        base = colname\n",
    "        while colname in tmp.columns:\n",
    "            colname = f\"{base}_{k}\"\n",
    "            k += 1\n",
    "        tmp[colname] = col_series(i)\n",
    "\n",
    "        # remember which colname maps to which year\n",
    "        year_col_map[colname] = year\n",
    "\n",
    "    # 4) melt to tidy\n",
    "    tidy = tmp.melt(id_vars=[\"label\"], var_name=\"year_col\", value_name=\"raw_value\")\n",
    "\n",
    "    # 5) map back to the actual year and parse numbers\n",
    "    tidy[\"year\"] = tidy[\"year_col\"].map(year_col_map).fillna(\"\")\n",
    "    tidy.dropna(subset=[\"year\"], inplace=True)\n",
    "    tidy[\"value\"] = tidy[\"raw_value\"].apply(parse_number)\n",
    "\n",
    "    # 6) final clean-up\n",
    "    tidy = tidy[[\"label\", \"year\", \"value\"]].reset_index(drop=True)\n",
    "    return tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4baae2b",
   "metadata": {},
   "source": [
    "### **Chroma helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma(collection_name: str = COLLECTION_NAME, persist_dir: str = CHROMA_DIR):\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "    vs = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    return vs\n",
    "\n",
    "\n",
    "def upsert_chunks_to_chroma(\n",
    "    chunks,\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    persist_dir: str = CHROMA_DIR,\n",
    "):\n",
    "    if not chunks:\n",
    "        return 0\n",
    "\n",
    "    vs = get_chroma(collection_name, persist_dir)\n",
    "\n",
    "    # Build Documents then filter complex metadata\n",
    "    docs = [Document(page_content=c[\"text\"], metadata=c[\"metadata\"]) for c in chunks]\n",
    "    docs = filter_complex_metadata(docs)\n",
    "\n",
    "    vs.add_documents(docs)\n",
    "\n",
    "    # For older langchain versions that still have .persist(), this is harmless.\n",
    "    try:\n",
    "        vs.persist()  # no-op on modern versions / AttributeError otherwise\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e914642",
   "metadata": {},
   "source": [
    "### **DuckDB helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76925af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DDL_METRICS = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS metrics (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    ticker TEXT,\n",
    "    filing_type TEXT,\n",
    "    fiscal_year INTEGER,\n",
    "    fiscal_period TEXT,\n",
    "    metric TEXT,\n",
    "    period_year INTEGER,\n",
    "    value DOUBLE,\n",
    "    units TEXT,\n",
    "    scale INTEGER,\n",
    "    source_filename TEXT,\n",
    "    page INTEGER,\n",
    "    table_id INTEGER,\n",
    "    provenance JSON\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "def init_duckdb(path: Path = DUCKDB_PATH) -> duckdb.DuckDBPyConnection:\n",
    "    #path.parent.mkdir(parents=True, exist_ok=True)  # <--- add this line\n",
    "    conn = duckdb.connect(str(path))\n",
    "    conn.execute(DDL_METRICS)\n",
    "    return conn\n",
    "\n",
    "\n",
    "def insert_metrics(conn: duckdb.DuckDBPyConnection, rows: List[Dict[str, Any]]):\n",
    "    if not rows:\n",
    "        return 0\n",
    "    df = pd.DataFrame(rows)\n",
    "    conn.register(\"df\", df)\n",
    "    conn.execute(\"INSERT OR REPLACE INTO metrics SELECT * FROM df\")\n",
    "    conn.unregister(\"df\")\n",
    "    return len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4aecf",
   "metadata": {},
   "source": [
    "### **Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class IngestStats:\n",
    "    chunk_docs: int = 0\n",
    "    metric_rows: int = 0\n",
    "\n",
    "\n",
    "def ingest_path(path: Path, ticker_default: Optional[str], conn: duckdb.DuckDBPyConnection) -> IngestStats:\n",
    "    stats = IngestStats()\n",
    "    base_meta = get_document_meta(path, ticker_default=ticker_default)\n",
    "\n",
    "    # ---- Vector text (Markdown) ----\n",
    "    try:\n",
    "        if path.suffix.lower() == \".pdf\":\n",
    "            md = pdf_to_markdown(path)\n",
    "        elif path.suffix.lower() in {\".docx\", \".doc\"}:\n",
    "            md = docx_to_markdown(path)\n",
    "        else:\n",
    "            print(f\"Skipping unsupported file: {path.name}\")\n",
    "            return stats\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to parse {path.name}: {e}\")\n",
    "        return stats\n",
    "          \n",
    "\n",
    "    chunks = md_to_chunks(md, base_meta)\n",
    "    stats.chunk_docs += upsert_chunks_to_chroma(chunks)\n",
    "\n",
    "    # ---- Extract metrics (PDF only) ----\n",
    "    if path.suffix.lower() == \".pdf\":\n",
    "        units, scale = scan_units_scale(md)\n",
    "        tables = extract_pdf_tables_with_provenance(path)\n",
    "\n",
    "        rows: List[Dict[str, Any]] = []\n",
    "        for t in tables:\n",
    "            tidy = melt_wide_years(t[\"df\"])\n",
    "            if tidy.empty:\n",
    "                continue\n",
    "            for _, r in tidy.iterrows():\n",
    "                metric_norm = best_metric_match(str(r[\"label\"]))\n",
    "                if not metric_norm:\n",
    "                    continue\n",
    "                val = r[\"value\"]\n",
    "                if val is None:\n",
    "                    continue\n",
    "                period_year = int(r[\"year\"]) if pd.notnull(r[\"year\"]) else None\n",
    "                rows.append({\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"ticker\": base_meta.get(\"ticker\"),\n",
    "                    \"filing_type\": base_meta.get(\"filing_type\"),\n",
    "                    \"fiscal_year\": int(base_meta[\"fiscal_year\"]) if base_meta.get(\"fiscal_year\") else None,\n",
    "                    \"fiscal_period\": base_meta.get(\"fiscal_period\"),\n",
    "                    \"metric\": metric_norm,\n",
    "                    \"period_year\": period_year,\n",
    "                    \"value\": float(val * (scale or 1)),\n",
    "                    \"units\": units,\n",
    "                    \"scale\": scale,\n",
    "                    \"source_filename\": base_meta.get(\"source_filename\"),\n",
    "                    \"page\": int(t[\"page\"]),\n",
    "                    \"table_id\": int(t[\"table_id\"]),\n",
    "                    \"provenance\": json.dumps({\n",
    "                        \"label_raw\": str(r[\"label\"]),\n",
    "                        \"columns_sample\": [str(c) for c in list(t[\"df\"].columns)[:6]],\n",
    "                        \"meta_provenance\": base_meta.get(\"meta_provenance\"),\n",
    "                    }),\n",
    "                })\n",
    "        stats.metric_rows += insert_metrics(conn, rows)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def cmd_ingest(data_dir, ticker):\n",
    "    data_dir = data_dir\n",
    "    ticker = ticker\n",
    "    conn = init_duckdb(DUCKDB_PATH)\n",
    "\n",
    "    files = [p for p in sorted(data_dir.glob(\"**/*\")) if p.suffix.lower() in (\".pdf\", \".docx\", \".doc\")]\n",
    "    if not files:\n",
    "        print(f\"No files found under {data_dir}.\")\n",
    "        return\n",
    "\n",
    "    total = IngestStats()\n",
    "    for f in files:\n",
    "        s = ingest_path(f, ticker_default=ticker, conn=conn)\n",
    "        total.chunk_docs += s.chunk_docs\n",
    "        total.metric_rows += s.metric_rows\n",
    "        print(f\"Ingested {f.name}: chunks+{s.chunk_docs}, metrics+{s.metric_rows}\")\n",
    "\n",
    "    print(\"—\" * 60)\n",
    "    print(f\"Total chunks added: {total.chunk_docs}\")\n",
    "    print(f\"Total metric rows upserted: {total.metric_rows}\")\n",
    "    print(f\"Chroma dir: {CHROMA_DIR}\")\n",
    "    print(f\"DuckDB file: {DUCKDB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_ingest(data_dir=Path(DEFAULT_DATA_DIR), ticker=\"MSFT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vizgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
